import os
import dotenv
from langsmith import traceable
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser

# Load environment vars
dotenv.load_dotenv()

LANGSMITH_API_KEY = os.getenv("LANGSMITH_API_KEY")
LANGSMITH_PROJECT = os.getenv("LANGSMITH_PROJECT")

# Path to generated HTML report
OUTPUT_HTML_PATH = "../outputs/job_market_report_20260112_213754.html"
with open(OUTPUT_HTML_PATH, "r", encoding="utf-8") as f:
    html_content = f.read()

# LLM setup
llm = ChatOpenAI(model="gpt-4-1106-preview", temperature=0)

# Prompt template
evaluation_prompt = ChatPromptTemplate.from_messages([
    ("system", "You are an expert evaluator of job market analysis reports."),
    ("user", """
You are given a job market analysis + resume boost HTML report generated by an AI pipeline.
Evaluate it based on the following criteria. Return your evaluation as JSON with keys: relevance, accuracy, completeness, clarity, visual_appeal, insights, and final_score (out of 10). Each should be scored out of 5.

Evaluation Criteria:
1. Relevance: Does the report reflect the requested job titles and country?
2. Accuracy: Are all mentioned skills/tools/platforms actually referenced in the job posts?
3. Completeness: Are all required sections (market overview, resume boost) present?
4. Clarity: Is the writing well-structured and easy to follow?
5. Visual Appeal: Does the HTML use a light theme, good styling, and easy-to-read tables?
6. Insights: Are the resume boost suggestions clearly justified?

Respond in the following JSON format:
{{
    "relevance": <score out of 5>,
    "accuracy": <score out of 5>,
    "completeness": <score out of 5>,
    "clarity": <score out of 5>,
    "visual_appeal": <score out of 5>,
    "insights": <score out of 5>,
    "final_score": <score out of 10>,
    "comments": "Optional general comments or suggestions"
}}

Here is the report content:
------------------
{html}
""")
]).partial(html=html_content)

parser = JsonOutputParser()

print("LANGCHAIN_API_KEY:", os.getenv("LANGCHAIN_API_KEY"))
print("LANGSMITH_PROJECT:", os.getenv("LANGSMITH_PROJECT"))
print("LANGSMITH_TRACING:", os.getenv("LANGSMITH_TRACING"))

# ✅ Traced function (auto-logs to LangSmith)
@traceable(name="HTML Report Evaluation")
def evaluate_html_report():
    chain = evaluation_prompt | llm | parser
    result = chain.invoke({})
    return result

# ✅ Just run and print
def main():
    result = evaluate_html_report()

    print("\n=== Evaluation Summary ===")
    for key, val in result.items():
        print(f"{key}: {val}")

if __name__ == "__main__":
    main()
